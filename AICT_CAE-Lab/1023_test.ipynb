{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f_data = np.loadtxt('test.csv', delimiter=',', dtype=np.float32)\n",
    "f_test = np.loadtxt('test2.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "# m_data = f_data[:, 0:1]\n",
    "# n_data = f_data[:, 1:2]\n",
    "# r_data = f_data[:, 2:3]\n",
    "# f_result = f_data[:, 3:4]\n",
    "# f_test1 = f_test[:]\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10000\n",
    "batch_size = 100\n",
    "display_step = 50\n",
    "\n",
    "x_data = f_data[:, 0:-1]\n",
    "y_data = f_data[:, [-1]]\n",
    "\n",
    "x_test = f_test[:, 0:-1]\n",
    "y_test = f_test[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 3) [[ 0.081       0.829       0.81699997]\n",
      " [ 0.96799999  0.89899999  0.162     ]\n",
      " [ 0.64700001  0.44600001  0.89399999]\n",
      " ..., \n",
      " [ 0.31200001  0.773       0.53100002]\n",
      " [ 0.926       0.294       0.199     ]\n",
      " [ 0.74299997  0.152       0.51899999]] 1000000\n",
      "(1000000, 1) [[  0.10059941]\n",
      " [ 33.15927505]\n",
      " [  0.3610473 ]\n",
      " ..., \n",
      " [  0.85500002]\n",
      " [  6.875     ]\n",
      " [  0.419     ]] 1000000\n",
      "(20000, 3) [[ 0.47142059  0.316365    0.55487716]\n",
      " [ 0.23590474  0.28930765  0.0159295 ]\n",
      " [ 0.44853005  0.50930876  0.14194755]\n",
      " ..., \n",
      " [ 0.39976066  0.42009386  0.29273492]\n",
      " [ 0.4884086   0.41115412  0.78293502]\n",
      " [ 0.77018207  0.16797483  0.9245882 ]] 20000\n",
      "(20000, 1) [[  4.84399021e-01]\n",
      " [  2.68962860e+02]\n",
      " [  1.13374872e+01]\n",
      " ..., \n",
      " [  1.95973492e+00]\n",
      " [  3.27594429e-01]\n",
      " [  1.51335523e-01]] 20000\n"
     ]
    }
   ],
   "source": [
    "# print(m_data.shape, m_data, len(m_data))\n",
    "# print(n_data.shape, n_data, len(n_data))\n",
    "# print(r_data.shape, r_data, len(r_data))\n",
    "# print(f_result.shape, f_result, len(f_result))\n",
    "# print(f_test1.shape, f_test1, len(f_test1))\n",
    "\n",
    "print(x_data.shape, x_data, len(x_data))\n",
    "print(y_data.shape, y_data, len(y_data))\n",
    "\n",
    "print(x_test.shape, x_test, len(x_test))\n",
    "print(y_test.shape, y_test, len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3,8]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([8]), name='bias1')\n",
    "layer1 = tf.matmul(X, W1) + b1\n",
    "\n",
    "# layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([8,8]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([8]), name='bias2')\n",
    "layer2 = tf.matmul(layer1, W2) + b2\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([8,6]), name='weight2')\n",
    "b3 = tf.Variable(tf.random_normal([6]), name='bias2')\n",
    "layer3 = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([6,6]), name='weight2')\n",
    "b4 = tf.Variable(tf.random_normal([6]), name='bias2')\n",
    "layer4 = tf.matmul(layer3, W4) + b4\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal([6,1]), name='weight2')\n",
    "b5 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.matmul(layer4, W5) + b5\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([3, 3]), name='weight2')\n",
    "# b2 = tf.Variable(tf.random_normal([3]), name='bias2')\n",
    "# layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([3, 3]), name='weight3')\n",
    "# b3 = tf.Variable(tf.random_normal([1]), name='bias3')\n",
    "# hypothesis = tf.nn.relu(tf.matmul(layer2, W3) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= nan W= [[ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]] b= [ nan  nan  nan  nan  nan  nan]\n",
      "Epoch: 0100 cost= nan W= [[ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]] b= [ nan  nan  nan  nan  nan  nan]\n",
      "Epoch: 0150 cost= nan W= [[ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan]] b= [ nan  nan  nan  nan  nan  nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c5dd0415eb49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Display logs per epoch step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JKU\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JKU\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JKU\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\JKU\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JKU\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "feed_dict = {X: x_data, Y: y_data}\n",
    "for epoch in range(training_epochs):\n",
    "    sess.run(optimizer, feed_dict)\n",
    "\n",
    "        # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        current_loss = sess.run(cost, feed_dict)\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(current_loss),\n",
    "            \"W=\", sess.run(W3), \"b=\", sess.run(b3))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "training_cost = sess.run(cost, feed_dict={X: x_data, Y: y_data})\n",
    "print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "\n",
    "\n",
    "# for step in range(1001):\n",
    "#     cost_val, hy_val, _ = sess.run(\n",
    "#         [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "#     if step % 10 == 0:\n",
    "#         print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "# # Ask my score\n",
    "# print(\"Your score will be \", sess.run(\n",
    "#     hypothesis, feed_dict={X: x_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "f_data = np.loadtxt('test.csv', delimiter=',', dtype=np.float32)\n",
    "f_test = np.loadtxt('test2.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very important. It does not work without it.\n",
    "f_data = MinMaxScaler(f_data)\n",
    "f_test = MinMaxScaler(f_test)\n",
    "print(f_data)\n",
    "print(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = f_data[:, 0:-1]\n",
    "y_data = f_data[:, [-1]]\n",
    "\n",
    "x_test = f_test[:, 0:-1]\n",
    "y_test = f_test[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3,4]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([4]), name='bias1')\n",
    "layer1 = tf.matmul(X, W1) + b1\n",
    "\n",
    "# layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([4,4]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([4]), name='bias2')\n",
    "layer2 = tf.matmul(layer1, W2) + b2\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([4,1]), name='weight2')\n",
    "b3 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([3, 3]), name='weight2')\n",
    "# b2 = tf.Variable(tf.random_normal([3]), name='bias2')\n",
    "# layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([3, 3]), name='weight3')\n",
    "# b3 = tf.Variable(tf.random_normal([1]), name='bias3')\n",
    "# hypothesis = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "\n",
    "# Hypothesis\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-4)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "    \n",
    "print(\"Learning finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: x_test, Y: y_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
